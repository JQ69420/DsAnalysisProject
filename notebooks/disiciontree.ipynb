{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy Scores for each fold: [0.67164179 0.64179104 0.67164179 0.66666667 0.71212121]\n",
      "Mean Accuracy: 0.6728\n",
      "Standard Deviation of Accuracy: 0.0226\n",
      "\n",
      "Precision Scores for each fold: [0.62857143 0.60606061 0.63636364 0.63636364 0.7       ]\n",
      "Mean Precision: 0.6415\n",
      "Standard Deviation of Precision: 0.0313\n",
      "\n",
      "Recall Scores for each fold: [0.70967742 0.64516129 0.67741935 0.67741935 0.67741935]\n",
      "Mean Recall: 0.6774\n",
      "Standard Deviation of Recall: 0.0204\n",
      "\n",
      "F1 Scores for each fold: [0.66666667 0.625      0.65625    0.65625    0.68852459]\n",
      "Mean F1: 0.6585\n",
      "Standard Deviation of F1: 0.0205\n",
      "\n",
      "Roc_auc Scores for each fold: [0.68100358 0.61648746 0.72132616 0.68248848 0.67695853]\n",
      "Mean Roc_auc: 0.6757\n",
      "Standard Deviation of Roc_auc: 0.0337\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "\n",
    "# Load the data\n",
    "student_file = os.path.join(\"..\", \"data\", \"processed\", \"Merged_Final_File_Updated.xlsx\")\n",
    "df = pd.read_excel(student_file)\n",
    "\n",
    "# Map dependent variable 'dropped out' to binary\n",
    "df['dropped out'] = df['dropped out'].map({'no': 0, 'yes': 1})\n",
    "\n",
    "# Define features and target\n",
    "features = ['anl1 final grade', 'anl2 final grade', 'anl3 final grade', 'anl4 final grade', 'education_level']\n",
    "target = 'dropped out'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Define preprocessing for numerical and categorical features\n",
    "numerical_features = ['anl1 final grade', 'anl2 final grade', 'anl3 final grade', 'anl4 final grade']\n",
    "categorical_features = ['education_level']\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=1)),  # Fill NA values with 1\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the pipeline with Decision Tree\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the Stratified K-Fold cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# List of metrics to evaluate\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "# Evaluate each metric using cross-validation\n",
    "results = {}\n",
    "for metric in metrics:\n",
    "    if metric == 'roc_auc':\n",
    "        # For ROC AUC, we need to use the 'predict_proba' method, so set the pipeline to use probabilities\n",
    "        scores = cross_val_score(pipeline, X, y, cv=cv, scoring='roc_auc')\n",
    "    else:\n",
    "        # For other metrics, use standard class predictions\n",
    "        scores = cross_val_score(pipeline, X, y, cv=cv, scoring=metric)\n",
    "        \n",
    "    results[metric] = {\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std(),\n",
    "        'scores': scores\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for metric, values in results.items():\n",
    "    print(f\"\\n{metric.capitalize()} Scores for each fold: {values['scores']}\")\n",
    "    print(f\"Mean {metric.capitalize()}: {values['mean']:.4f}\")\n",
    "    print(f\"Standard Deviation of {metric.capitalize()}: {values['std']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
